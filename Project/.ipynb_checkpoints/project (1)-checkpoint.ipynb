{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from fa2 import ForceAtlas2\n",
    "import glob\n",
    "import os\n",
    "import nltk, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import FreqDist \n",
    "import wordcloud \n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdbId_to_wikiLink(imdbId):\n",
    "\n",
    "    imdbId = str(imdbId)\n",
    "    \n",
    "    while len(imdbId)<7:\n",
    "        imdbId = str(0) + imdbId  \n",
    "\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = '''\n",
    "    SELECT ?wppage WHERE {                                                          \n",
    "    ?subject wdt:P345 'tt''' + imdbId + '''' .                                                   \n",
    "      ?wppage schema:about ?subject .                                               \n",
    "      FILTER(contains(str(?wppage),'//en.wikipedia'))                               \n",
    "    }\n",
    "    '''\n",
    "    r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    while (r.status_code == 429):\n",
    "        time.sleep(0.1)\n",
    "        r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    if (len(data['results']['bindings']) == 0):\n",
    "        return None\n",
    "        \n",
    "    weblink = data['results']['bindings'][0]['wppage']['value']\n",
    "\n",
    "    \n",
    "    wikilink = ''\n",
    "    \n",
    "    #match the last /wikilink part of url\n",
    "    regex = re.compile(r\".+\\/(.+)\")\n",
    "    wikimatch = re.search(regex, weblink)\n",
    "    \n",
    "    if wikimatch is not None:\n",
    "        wikilink = wikimatch.group(1)\n",
    "    \n",
    "    return wikilink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikiLink_to_wikiText(wikiLink):\n",
    "\n",
    "    #as described at https://www.mediawiki.org/wiki/API:Tutorial\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataformat =\"format=json\"\n",
    "    \n",
    "    title = \"titles={}\".format(wikilink)\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    \n",
    "    return wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_in_common(movie1, movie2):\n",
    "    \n",
    "    #define the to list of genres\n",
    "    G1 = G.nodes[movie1]['genres']\n",
    "    G2 = G.nodes[movie2]['genres']\n",
    "    \n",
    "    #make list of genres in common\n",
    "    common = [i for i in G1 if i in G2]\n",
    "    \n",
    "    if (len(common)>0):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggestMovie(movieId):\n",
    "    \n",
    "    #used to keep track of strongest link to other movie\n",
    "    max_weight = 0\n",
    "    \n",
    "    for edge in G.edges(movieId):\n",
    "        \n",
    "        #if link is the strongest we've seen, then save it\n",
    "        if G[edge[0]][edge[1]]['weight'] > max_weight:\n",
    "            max_weight = G[edge[0]][edge[1]]['weight']\n",
    "            recommended_edge = edge\n",
    "    \n",
    "    #return the recommended movie from the strongest link\n",
    "    if recommended_edge[0] != movieId:\n",
    "        recommended_movie = recommended_edge[0]\n",
    "    else:\n",
    "        recommended_movie = recommended_edge[1]\n",
    "        \n",
    "    return G.nodes[recommended_movie]['title']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yearly_avg(year):\n",
    "    \n",
    "    #used to calculate average\n",
    "    count = 0\n",
    "    sum = 0\n",
    "    \n",
    "    for movie in G.nodes:\n",
    "        \n",
    "        #match numbers in ()\n",
    "        match = re.search(r\"\\(([0-9]*?)\\)\", G.nodes[movie]['title'])\n",
    "        \n",
    "        if (match is not None):\n",
    "            \n",
    "            #if movie is from year, update count and sum\n",
    "            if (int(match.group(1)) == year):\n",
    "                count += 1\n",
    "                sum += G.nodes[movie]['avg_rating']\n",
    "                \n",
    "    if (count == 0):\n",
    "        return 0\n",
    "    return (sum/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieId_to_filetitle(movieId):\n",
    "    \n",
    "    illegal = ['#','%','&','{','}','\\\\','$','!','\\'','\"',':','<','>','*','?','/',' ','+','`','|','=']\n",
    "\n",
    "    fileTitle = G.nodes[movieId]['title'] + '.txt'\n",
    "    \n",
    "    for sign in illegal:\n",
    "        fileTitle = fileTitle.replace(sign,'_')\n",
    "    return fileTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "readerLinks = pd.read_csv('links.csv')\n",
    "readerMovies = pd.read_csv('movies.csv')\n",
    "readerRatings = pd.read_csv('ratings.csv')\n",
    "#readerRatings =pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for i in range(len(readerLinks)):\n",
    "\n",
    "    movieId = readerLinks.iloc[i,0]\n",
    "    imdbId = readerLinks.iloc[i,1]\n",
    "    \n",
    "    links.append([movieId, imdbId])\n",
    "movies = []\n",
    "for i in range(len(readerMovies)):\n",
    "\n",
    "    movieId = readerMovies.iloc[i,0]\n",
    "    titel = readerMovies.iloc[i,1]\n",
    "    genres = readerMovies.iloc[i,2]\n",
    "    \n",
    "    movies.append([movieId, titel, genres])\n",
    "ratings = []\n",
    "for i in range(len(readerRatings)):\n",
    "\n",
    "    userId = readerRatings.iloc[i,0]\n",
    "    movieId = readerRatings.iloc[i,1]\n",
    "    rating = readerRatings.iloc[i,2]\n",
    "    \n",
    "    ratings.append([userId, movieId, rating])\n",
    "G = nx.Graph()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', 'Grumpier Old Men (1995)', 'Comedy|Romance']\n"
     ]
    }
   ],
   "source": [
    "print(movies[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movies:\n",
    "    print(movie[2])\n",
    "    #make list of genres from string of genres\n",
    "    genres = re.findall(r\"([^|]+)\", movie[2])\n",
    "    \n",
    "    G.add_node(movie[0], title=movie[1], avg_rating=0, rating_count=0, genres=genres,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "limit_for_love = 4\n",
    "\n",
    "#for every movie loved by user, make a link/increase weight of existing link, to movies also loved by user\n",
    "for i in range(len(ratings)):\n",
    "    \n",
    "    #defining movie\n",
    "    movie = ratings[i][1]\n",
    "    \n",
    "    #update average rating of movie\n",
    "    old_average = G.nodes[movie]['avg_rating']\n",
    "    old_count = G.nodes[movie]['rating_count']\n",
    "    \n",
    "    G.nodes[movie]['rating_count'] += 1\n",
    "    G.nodes[movie]['avg_rating'] = (old_average*old_count + ratings[i][2])/G.nodes[movie]['rating_count']\n",
    "    \n",
    "    #if user doesnt love movie dont bother linking to it\n",
    "    if (ratings[i][2] < limit_for_love):\n",
    "        continue\n",
    "    \n",
    "    #increase index to avoid linking to itself\n",
    "    j = i+1\n",
    "    \n",
    "    #avoid out of range\n",
    "    if(j < len(ratings)):\n",
    "        \n",
    "        #look at all other ratings made by same user\n",
    "        while (ratings[j][0] == ratings[i][0]):\n",
    "            \n",
    "            second_movie = ratings[j][1]\n",
    "            \n",
    "            #if the user loves second_movie and the two movies share a genre, add an edge between them or increase weight of existing\n",
    "            if (ratings[j][2] >= limit_for_love and genre_in_common(movie, second_movie)):\n",
    "                if (G.has_edge(movie, second_movie)):\n",
    "                    G[movie][second_movie]['weight'] += 1 #increase weight on existing edge\n",
    "                else:\n",
    "                    G.add_edge(movie, ratings[j][1], weight=1) #or create new edge\n",
    "            j+=1\n",
    "            if (j>=len(ratings)):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(51):\n",
    "    loved = 0\n",
    "    for node in G.nodes:\n",
    "        if (G.nodes[node]['avg_rating'] >= limit_for_love and G.nodes[node]['rating_count'] > i):\n",
    "            loved += 1\n",
    "    x.append(i)\n",
    "    y.append(loved)\n",
    "\n",
    "plt.plot(x, y)\n",
    "  \n",
    "plt.xlabel('minimum ratings')\n",
    "plt.ylabel('count of loved movies')\n",
    "  \n",
    "plt.title('How many ratings does a loved movie have?')\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weakly_connected_components only works on undirected graphs\n",
    "G_directed = G.to_directed()\n",
    "Gsub = max(nx.weakly_connected_components(G_directed))\n",
    "Gsub = nx.subgraph(G, Gsub)\n",
    "Gsub = Gsub.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_colors = []\n",
    "\n",
    "red = \"#A83432\"\n",
    "blue = \"#323EA8\"\n",
    "\n",
    "for node in Gsub:\n",
    "    #color movies liked on average red\n",
    "    if G.nodes[node]['avg_rating'] >= limit_for_love and G.nodes[node]['rating_count'] > 20:\n",
    "        defined_colors.append(red)\n",
    "    #color the rest blue\n",
    "    else:\n",
    "        defined_colors.append(blue)\n",
    "forceatlas2 = ForceAtlas2(gravity = 1.0)\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(Gsub, pos=None, iterations= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(200, 100), dpi=60)\n",
    "nodes = nx.draw_networkx_nodes(Gsub, positions, alpha=1, node_color = defined_colors)\n",
    "nx.draw_networkx_edges(Gsub, positions, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B TF-IDF\n",
    "\n",
    "For each given category, the movies corresponding the wikitext are combined into one document. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import string \n",
    "\n",
    "def get_plot(file):   \n",
    "    # We only analyse the plot of the movie. \n",
    "    \n",
    "    if file.find(\"==Plot==\") is not -1 and file.find(\"==Cast==\") is not -1 :\n",
    "        \n",
    "        min = file.find(\"==Plot==\")\n",
    "        max = file.find(\"==Cast==\")\n",
    "        plot_file = file[min:max]\n",
    "            \n",
    "    elif file.find(\"References\") is not -1:\n",
    "        \n",
    "        max = file.find(\"References\")\n",
    "        plot_file = file[1000:max]\n",
    "           \n",
    "    else: \n",
    "         \n",
    "        plot_file = file\n",
    "    \n",
    "    return plot_file\n",
    "    \n",
    "                    \n",
    "def cleaning_text(raw):\n",
    "    \n",
    "    words = nltk.wordpunct_tokenize(raw)\n",
    "    \n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*') # must contain a letter or digit\n",
    "    filter  = [w for w in words if nonPunct.match(w)]\n",
    "    \n",
    "    #import enchant\n",
    "\n",
    "    #d = enchant.Dict(\"en_US\")\n",
    "\n",
    "    words = [word.lower() for word in filter]\n",
    "    \n",
    "\n",
    "    #words = [i for i in words if len(i) > 1]\n",
    "            \n",
    "    \n",
    "    return words\n",
    "    \n",
    "def remove_unwanted(words):\n",
    "     \n",
    "    stops= stopwords.words('english')\n",
    "    months = ['january','february','march','april','may','june','july','august','september','october','november']\n",
    "    Extra_stops = ['plot', 'nin', 'ref','url','www','http','https','com','jsonline','df','n','cite','web','html','mdy','name','article','title','date','webcitation','acces']\n",
    "    Extra = ['webb','film','nmark','critic','critics','publisher','director','editor','deschanel','awards','aa','list','plainlist']\n",
    "    unwanted = stops  + Extra_stops + months + Extra\n",
    "    \n",
    "    \n",
    "    #final  = [word for word in words if word.lower() not in stops]\n",
    "    final = []\n",
    "    for word in words:\n",
    "        if word not in unwanted:\n",
    "           final.append(word)\n",
    "           \n",
    "    final = \" \".join(final)\n",
    "    final = final.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    final = \"\".join([i for i in final if not i.isdigit()]) # removing year and such\n",
    "    \n",
    "    while \"  \" in final:\n",
    "        \n",
    "        final = final.replace(\"  \", \" \")\n",
    "    \n",
    "    return final \n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "stops= stopwords.words('english')\n",
    "months = ['january','february','march','april','may','june','july','august','september','october','november']\n",
    "Extra_stops = ['plot', 'nin', 'ref','url','www','http','https','com','jsonline','df','n','cite','web','html','mdy','name','article','title','date','webcitation','access']\n",
    "stops = stops + Extra_stops + months \n",
    "\n",
    "print(Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "Documents = [None]*3\n",
    "Document = []\n",
    "\n",
    "# iterate over files in\n",
    "Wikitext = os.listdir(os.getcwd() + '\\\\Wikitexts')\n",
    "count = 0\n",
    "for text in Wikitext:\n",
    "   if count > 13:\n",
    "      break\n",
    "   else:\n",
    "      \n",
    "      if count <= 4:\n",
    "         with open(os.getcwd() + '\\\\Wikitexts\\\\' + text) as f:\n",
    "            file = f.read()\n",
    "            file = get_plot(file)\n",
    "            file_word = remove_unwanted(cleaning_text(file))\n",
    "            \n",
    "            #Documents[0] = Documents[0] + ' ' + str(file_word)\n",
    "            #Documents[0] = file_word\n",
    "            Document.append(file_word)\n",
    "            \n",
    "      elif 4<count<=8:\n",
    "         with open(os.getcwd() + '\\\\Wikitexts\\\\' + text) as f:\n",
    "            file = f.read()\n",
    "            file = get_plot(file)\n",
    "            file_word = remove_unwanted(cleaning_text(file))\n",
    "            #Documents[1] = Documents[1] + ' ' +str(file_word)\n",
    "            Documents[1] = file_word\n",
    "            Document.append(file_word)\n",
    "           \n",
    "      elif 8<count<=12:\n",
    "         with open(os.getcwd() + '\\\\Wikitexts\\\\' + text) as f:\n",
    "            file = f.read()\n",
    "            file = get_plot(file)\n",
    "            file_word = remove_unwanted(cleaning_text(file))\n",
    "            #Documents[2].append(file_word)\n",
    "            Documents[2] = file_word\n",
    "            Document.append(str(file_word))\n",
    "            \n",
    "            #doc2 = doc2 + ' ' + file\n",
    "   count = count+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "Documents = ['','','','','','','','','','','','','']\n",
    "Document = []\n",
    "\n",
    "# iterate over files in\n",
    "Wikitext = os.listdir(os.getcwd() + '\\\\Wikitexts')\n",
    "count = 0\n",
    "for text in Wikitext:\n",
    "      \n",
    "   \n",
    "      if count < 13:\n",
    "         with open(os.getcwd() + '\\\\Wikitexts\\\\' + text) as f:\n",
    "            file = f.read()\n",
    "            file = get_plot(file)\n",
    "            file_word = remove_unwanted(cleaning_text(file))\n",
    "            \n",
    "            Documents[count] = Documents[count] + ' ' + file_word\n",
    "            #Documents[0] = file_word\n",
    "            #Document.append(file_word)\n",
    "            \n",
    "     \n",
    "            #doc2 = doc2 + ' ' + file\n",
    "      count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello dark and brownwwwcourse the I lived 2065. you live in words legged-skin and soft www\n",
      "hello dark brownwwwcourse lived  live words legged skin soft\n"
     ]
    }
   ],
   "source": [
    "h = 'Hello dark and brown wwwcourse the I lived 2065. you live in words legged-skin and soft www'\n",
    "\n",
    "print(h)\n",
    "print(remove_unwanted(cleaning_text(h)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bleach\n",
    "import requests\n",
    "\n",
    "with open(\"C:/Users/Mabel Ifeoma/Socialgraphs/Wikitexts/Zookeeper_(2011).txt\") as f:\n",
    "    test = f.read()\n",
    "\n",
    "#test\n",
    "#print(test.find('plot'))\n",
    "\n",
    "with open(\"C:/Users/Mabel Ifeoma/Socialgraphs/Wikitexts/Young_Adult_(2011).txt\") as f:\n",
    "    test2 = f.read()\n",
    "    \n",
    "min = test.find(\"==Plot==\")\n",
    "max = test.find(\"==Cast==\")\n",
    "\n",
    "\n",
    "test = test[min:max]\n",
    "test = remove_unwanted(cleaning_text(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MABELI~1\\AppData\\Local\\Temp/ipykernel_15340/3232608838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munwanted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mfeuture_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         \"\"\"\n\u001b[0;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1310\u001b[1;33m                 \u001b[1;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m             )\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "# We use function to create the TF-IDF vectors\n",
    "#calling the TfidfVectorizer\n",
    "\n",
    "stops= stopwords.words('english')\n",
    "months = ['january','february','march','april','may','june','july','august','september','october','november']\n",
    "Extra_stops = ['plot', 'nin', 'ref','url','www','http','https','com','jsonline','df','n','cite','web','html','mdy','name','article','title','date','webcitation','access']\n",
    "Extra = ['webb','film','nmark','critic','critics','publisher','director','editor','deschanel','awards','aa']\n",
    "unwanted = stops  + Extra_stops + months + Extra\n",
    "\n",
    "vectorize= TfidfVectorizer(stop_words=unwanted, ngram_range= (1,1)) \n",
    "response= vectorize.fit_transform(Documents)\n",
    "\n",
    "feuture_names = vectorize.get_feature_names_out()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "from nltk.corpus import words\n",
    "dense = response.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "all_keywords = []\n",
    "\n",
    "for description in denselist:\n",
    "    x=0\n",
    "    keywords = []\n",
    "    for word in description:\n",
    "       \n",
    "        if word > 0 :\n",
    "                \n",
    "            \n",
    "            keywords.append(feuture_names[x])\n",
    "        x=x+1\n",
    "    all_keywords.append(keywords)\n",
    "\n",
    "\n",
    "#print (Documents[0])\n",
    "print (all_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "true_k = 8\n",
    "\n",
    "model = KMeans(n_clusters=true_k, init=\"k-means++\", max_iter=100, n_init=1)\n",
    "\n",
    "model.fit(response[0])\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorize.get_feature_names_out()\n",
    "\n",
    "with open (\"C:/Users/Mabel Ifeoma/Socialgraphs/trial_result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids[i, :15]:\n",
    "            f.write (' %s' % terms[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MABELI~1\\AppData\\Local\\Temp/ipykernel_15512/3354592176.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclean_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_text_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaning_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#clean_text1, clean_text_str1 = cleaning_text(get_plot(test2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MABELI~1\\AppData\\Local\\Temp/ipykernel_15512/1022029135.py\u001b[0m in \u001b[0;36mcleaning_text\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Join it into one text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;34m\"  \"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mclean_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_text_str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
